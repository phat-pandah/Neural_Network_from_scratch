{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import time\r\n",
    "\r\n",
    "print('Import successful!!!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Import successful!!!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# We well implement a 4 layer model\r\n",
    "\r\n",
    "\r\n",
    "class DeepNeuralNetwork():\r\n",
    "    def __init__(self, sizes, epochs=25, learn_rate=0.001):\r\n",
    "        self.sizes = sizes\r\n",
    "        self.epochs = epochs\r\n",
    "        self.learn_rate = learn_rate\r\n",
    "\r\n",
    "        # Save all parms\r\n",
    "        self.params = self.init()\r\n",
    "\r\n",
    "    # Sigmoid and softmax are the 2 activation functions used\r\n",
    "    # sigmoid for hidden layers, softmax for output layer\r\n",
    "    def sigmoid(self, x, deriv=False):\r\n",
    "        if deriv:\r\n",
    "            return np.exp(-x) / ((np.exp(-x)+1)**2)\r\n",
    "\r\n",
    "        return 1 / (1+np.exp(-x))\r\n",
    "\r\n",
    "    def softmax(self, x, deriv=False):\r\n",
    "        e = np.exp(x - x.max())\r\n",
    "        if deriv:\r\n",
    "            return e / np.sum(e, axis=0) * (1 - e / np.sum(e, axis=0))\r\n",
    "\r\n",
    "        return e / np.sum(e, axis=0)\r\n",
    "\r\n",
    "    def init(self):\r\n",
    "        input_layer = self.sizes[0]\r\n",
    "        hidden1 = self.sizes[1]\r\n",
    "        hidden2 = self.sizes[2]\r\n",
    "        output_layer = self.sizes[3]\r\n",
    "\r\n",
    "        params = {\r\n",
    "            'W1': np.random.randn(hidden1, input_layer) * np.sqrt(1. / hidden1),\r\n",
    "            'W2': np.random.randn(hidden2, hidden1) * np.sqrt(1. / hidden2),\r\n",
    "            'W3': np.random.randn(output_layer, hidden2) * np.sqrt(1. / output_layer)\r\n",
    "        }\r\n",
    "\r\n",
    "        return params\r\n",
    "\r\n",
    "    def forward_propagation(self, x_train):\r\n",
    "        params = self.params\r\n",
    "\r\n",
    "        # input layer activations becomes sample\r\n",
    "        params['A0'] = x_train\r\n",
    "\r\n",
    "        # input layer to hidden layer 1\r\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\r\n",
    "        params['A1'] = self.sigmoid(params['Z1'])\r\n",
    "\r\n",
    "        # hidden layer 1 to hidden layer 2\r\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\r\n",
    "        params['A2'] = self.sigmoid(params['Z2'])\r\n",
    "\r\n",
    "        # hidden layer 2 to output layer\r\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\r\n",
    "        params['A3'] = self.softmax(params['Z3'])\r\n",
    "\r\n",
    "        return params['A3']\r\n",
    "\r\n",
    "    def backwards_propagation(self, y_train, output):\r\n",
    "        params = self.params\r\n",
    "        change_W = {}\r\n",
    "\r\n",
    "        # Update W3\r\n",
    "        error = 2 * (output - y_train) / \\\r\n",
    "            output.shape[0] * self.softmax(params['Z3'], deriv=True)\r\n",
    "        change_W['W3'] = np.outer(error, params['A2'])\r\n",
    "\r\n",
    "        # Update W2\r\n",
    "        error = np.dot(params['W3'].T, error) * \\\r\n",
    "            self.sigmoid(params['Z2'], deriv=True)\r\n",
    "        change_W['W2'] = np.outer(error, params['A1'])\r\n",
    "\r\n",
    "        # Update W1\r\n",
    "        error = np.dot(params['W2'].T, error) * \\\r\n",
    "            self.sigmoid(params['Z1'], deriv=True)\r\n",
    "        change_W['W1'] = np.outer(error, params['A0'])\r\n",
    "\r\n",
    "        return change_W\r\n",
    "\r\n",
    "    def gradient_descent(self, W_change):\r\n",
    "        for key, val in W_change.items():\r\n",
    "            self.param[key] -= self.learn_rate * val\r\n",
    "\r\n",
    "    def accuracy(self, x_val, y_val):\r\n",
    "        predictions = []\r\n",
    "\r\n",
    "        for x, y in zip(x_val, y_val):\r\n",
    "            out = self.forward_prpagation(x)\r\n",
    "            pred = np.argmax(out)\r\n",
    "            predictions.append(pred == np.argmax(y))\r\n",
    "\r\n",
    "        return np.mean(predictions)\r\n",
    "\r\n",
    "    def train(self, x_train, y_train, x_val, y_val):\r\n",
    "        start_time = time.time()\r\n",
    "        \r\n",
    "        for iter in range(self.epochs):\r\n",
    "            for x, y in zip(x_train, y_train):\r\n",
    "                out = self.forward_propagation(x)\r\n",
    "                w_changes = self.backwards_propagation(y, out)\r\n",
    "                self.gradient_descent(w_changes)\r\n",
    "                \r\n",
    "            acc = accuracy(x_val, y_val)\r\n",
    "            \r\n",
    "            print(f'Epoch: {itter+1}, Time Spent: {time.time() - start_time:.2f}, Accuracy{acc * 100:.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "dnn = DeepNeuralNetwork(sizes=[784, 128, 64, 10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Using MNIST dataset\r\n",
    "\r\n",
    "WE will test the neural network using the MNIST data set. This dataset contains 60 000 grayscale images that are 28 x 28 pixels, which consist of handwritten digits (0 to 10). We will flatten the images before inputting them into the network. So the input layer will a rank 1 tensor of length 728."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.datasets import fetch_openml\r\n",
    "from tensorflow.keras.utils.np_utils import to_categorical\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# import dataset and prepare to feed it to network\r\n",
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\r\n",
    "x = (x/255).astype('float32')\r\n",
    "y = to_categorical(y)\r\n",
    "\r\n",
    "# split data\r\n",
    "x_train, x_val, y_tain, y_val = train_test_split(x, y, test_size=.10, random_state= 42)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b19c1e8ed67b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import dataset and prepare to feed it to network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import tensorflow as tf\r\n",
    "tf.__version__"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-75de35cf8cd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}